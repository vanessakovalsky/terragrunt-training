# Exercice Terragrunt - Pipeline CI/CD SÃ©curisÃ© (60 minutes)

## ğŸ¯ Objectif
CrÃ©er un pipeline CI/CD complet et sÃ©curisÃ© pour automatiser les dÃ©ploiements Terragrunt avec validation, planification, dÃ©ploiement conditionnel et sauvegarde des logs.

## ğŸ“‹ PrÃ©requis
- Repository Git (GitHub/GitLab)
- AWS CLI et credentials configurÃ©s
- Terragrunt/Terraform installÃ©s
- AccÃ¨s Ã  un service CI/CD (GitHub Actions, GitLab CI)

## ğŸ—ï¸ Architecture du Pipeline
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Validation    â”‚ -> â”‚   Plan Stage    â”‚ -> â”‚  Deploy Stage   â”‚
â”‚   - Syntax      â”‚    â”‚   - All modules â”‚    â”‚   - Main only   â”‚
â”‚   - Linting     â”‚    â”‚   - Security    â”‚    â”‚   - With logs   â”‚
â”‚   - Format      â”‚    â”‚   - Cost        â”‚    â”‚   - Monitoring  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```


## Phase 1 : Structure Projet et Validation (15 minutes)

### 1.1 Structure du Projet
```
secure-terragrunt-pipeline/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ validate.yml
â”‚       â”œâ”€â”€ plan.yml
â”‚       â””â”€â”€ deploy.yml
â”œâ”€â”€ .gitlab-ci.yml
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ validate.sh
â”‚   â”œâ”€â”€ plan-all.sh
â”‚   â”œâ”€â”€ deploy.sh
â”‚   â””â”€â”€ save-logs.sh
â”œâ”€â”€ environments/
â”‚   â”œâ”€â”€ dev/
â”‚   â”‚   â”œâ”€â”€ terragrunt.hcl
â”‚   â”‚   â”œâ”€â”€ vpc/
â”‚   â”‚   â”œâ”€â”€ security/
â”‚   â”‚   â””â”€â”€ compute/
â”‚   â”œâ”€â”€ staging/
â”‚   â””â”€â”€ prod/
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ networking/
â”‚   â”œâ”€â”€ security/
â”‚   â””â”€â”€ compute/
â”œâ”€â”€ policies/
â”‚   â”œâ”€â”€ sentinel/
â”‚   â””â”€â”€ opa/
â””â”€â”€ docs/
    â””â”€â”€ pipeline.md
```

### 1.2 Configuration Root Terragrunt
```hcl
# environments/terragrunt.hcl
locals {
  # Variables d'environnement
  environment = basename(get_terragrunt_dir())
  region      = get_env("AWS_DEFAULT_REGION", "eu-west-1")
  project     = "secure-pipeline"
  
  # Configuration par environnement
  env_vars = {
    dev = {
      instance_type = "t3.micro"
      min_size     = 1
      max_size     = 2
    }
    staging = {
      instance_type = "t3.small"
      min_size     = 2
      max_size     = 4
    }
    prod = {
      instance_type = "t3.medium"
      min_size     = 3
      max_size     = 10
    }
  }
  
  # Tags de sÃ©curitÃ©
  security_tags = {
    Environment     = local.environment
    Project        = local.project
    ManagedBy      = "terragrunt"
    SecurityLevel  = local.environment == "prod" ? "high" : "medium"
    DataClass      = "internal"
    Owner          = get_env("PIPELINE_OWNER", "devops-team")
    CostCenter     = get_env("COST_CENTER", "engineering")
    DeployedBy     = get_env("CI_PIPELINE_ID", "manual")
    DeployedAt     = formatdate("YYYY-MM-DD-hhmm", timestamp())
  }
}

# Configuration du backend avec chiffrement
remote_state {
  backend = "s3"
  config = {
    bucket = "secure-terragrunt-state-${local.environment}"
    key    = "${path_relative_to_include()}/terraform.tfstate"
    region = local.region
    
    # SÃ©curitÃ© renforcÃ©e
    encrypt                = true
    kms_key_id            = "arn:aws:kms:${local.region}:${get_aws_account_id()}:key/pipeline-state"
    dynamodb_table        = "terragrunt-locks-${local.environment}"
    skip_bucket_versioning = false
    
    # Politique de rÃ©tention
    lifecycle_configuration = {
      rule = {
        id     = "state_retention"
        status = "Enabled"
        
        noncurrent_version_expiration = {
          days = 90
        }
      }
    }
  }
}

# Provider avec contraintes de sÃ©curitÃ©
generate "provider" {
  path      = "provider.tf"
  if_exists = "overwrite_terragrunt"
  contents  = <<EOF
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
  required_version = ">= 1.0"
}

provider "aws" {
  region = "${local.region}"
  
  # Contraintes de sÃ©curitÃ©
  allowed_account_ids = ["${get_aws_account_id()}"]
  
  default_tags {
    tags = ${jsonencode(local.security_tags)}
  }
  
  # Assume role pour sÃ©curitÃ© accrue
  assume_role {
    role_arn = "arn:aws:iam::${get_aws_account_id()}:role/TerragruntDeployRole-${title(local.environment)}"
  }
}
EOF
}

# Hooks de sÃ©curitÃ© globaux
terraform {
  before_hook "security_check" {
    commands = ["plan", "apply"]
    execute = [
      "bash", "-c", <<-EOT
        echo "ğŸ” Running security checks..."
        # VÃ©rification des credentials
        aws sts get-caller-identity
        # VÃ©rification de l'environnement
        if [[ "${local.environment}" == "prod" && "${get_env("CI_COMMIT_REF_NAME", "")}" != "main" ]]; then
          echo "âŒ Production deployments only allowed from main branch"
          exit 1
        fi
      EOT
    ]
  }
  
  before_hook "cost_estimation" {
    commands = ["plan"]
    execute = [
      "bash", "-c", <<-EOT
        echo "ğŸ’° Estimating infrastructure costs..."
        # Integration avec Infracost si disponible
        if command -v infracost &> /dev/null; then
          infracost breakdown --path=.
        fi
      EOT
    ]
  }
  
  after_hook "compliance_check" {
    commands = ["apply"]
    execute = [
      "bash", "-c", <<-EOT
        echo "ğŸ“‹ Running compliance checks..."
        # VÃ©rification des tags obligatoires
        aws resourcegroupstaggingapi get-resources --region ${local.region} \
          --tag-filters Key=Environment,Values=${local.environment} \
          --query 'ResourceTagMappingList[?!Tags[?Key==`Owner`]]' || true
      EOT
    ]
  }
}
```

### 1.3 Scripts de Validation
```bash
# scripts/validate.sh
#!/bin/bash

set -euo pipefail

echo "ğŸ” Starting Terragrunt validation pipeline..."

# Configuration
LOG_DIR="logs"
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
VALIDATION_LOG="$LOG_DIR/validation_$TIMESTAMP.log"

# CrÃ©ation du dossier de logs
mkdir -p "$LOG_DIR"

# Fonction de logging
log() {
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1" | tee -a "$VALIDATION_LOG"
}

# Fonction de validation avec code de retour
validate_step() {
    local step_name="$1"
    local command="$2"
    
    log "ğŸ“ Running: $step_name"
    
    if eval "$command" >> "$VALIDATION_LOG" 2>&1; then
        log "âœ… $step_name: PASSED"
        return 0
    else
        log "âŒ $step_name: FAILED"
        return 1
    fi
}

# Variables d'environnement requises
required_vars=(
    "AWS_DEFAULT_REGION"
    "AWS_ACCESS_KEY_ID" 
    "AWS_SECRET_ACCESS_KEY"
)

# Validation des variables d'environnement
log "ğŸ” Checking required environment variables..."
for var in "${required_vars[@]}"; do
    if [[ -z "${!var:-}" ]]; then
        log "âŒ Missing required environment variable: $var"
        exit 1
    fi
    log "âœ… $var is set"
done

# 1. Validation de la syntaxe Terragrunt
log "ğŸ” Phase 1: Syntax Validation"
validate_step "Terragrunt syntax check" "find . -name '*.hcl' -exec terragrunt hclfmt --terragrunt-check {} \;"

# 2. Validation Terraform
log "ğŸ” Phase 2: Terraform Validation"
environments=("dev" "staging" "prod")

for env in "${environments[@]}"; do
    if [[ -d "environments/$env" ]]; then
        log "ğŸ“ Validating environment: $env"
        cd "environments/$env"
        
        # Validation de tous les modules
        modules=$(find . -name "terragrunt.hcl" -not -path "./terragrunt.hcl" | xargs dirname)
        
        for module in $modules; do
            log "ğŸ” Validating module: $env/$module"
            cd "$module"
            
            validate_step "Init $env/$module" "terragrunt init --terragrunt-non-interactive"
            validate_step "Validate $env/$module" "terragrunt validate"
            validate_step "Format check $env/$module" "terraform fmt -check=true -diff=true"
            
            cd - > /dev/null
        done
        
        cd - > /dev/null
    fi
done

# 3. SÃ©curitÃ© et conformitÃ©
log "ğŸ” Phase 3: Security & Compliance"

# Scan des secrets avec git-secrets ou truffleHog
if command -v git-secrets &> /dev/null; then
    validate_step "Secret scan" "git secrets --scan"
elif command -v truffleHog &> /dev/null; then
    validate_step "Secret scan" "truffleHog --regex --entropy=False ."
else
    log "âš ï¸ No secret scanning tool found (git-secrets or truffleHog recommended)"
fi

# Scan de sÃ©curitÃ© avec tfsec
if command -v tfsec &> /dev/null; then
    validate_step "Security scan" "tfsec --format json --out $LOG_DIR/security_$TIMESTAMP.json ."
    log "ğŸ“Š Security report saved to: $LOG_DIR/security_$TIMESTAMP.json"
else
    log "âš ï¸ tfsec not found - security scanning skipped"
fi

# Validation des policies avec OPA/Sentinel (si disponible)
if [[ -d "policies/opa" ]] && command -v opa &> /dev/null; then
    validate_step "OPA policy validation" "opa test policies/opa/"
fi

# 4. GÃ©nÃ©ration du rapport
log "ğŸ“Š Phase 4: Report Generation"

# RÃ©sumÃ© des validations
cat << EOF > "$LOG_DIR/validation_summary_$TIMESTAMP.md"
# Validation Report

**Date:** $(date)
**Commit:** \${GITHUB_SHA:-\${CI_COMMIT_SHA:-$(git rev-parse HEAD)}}
**Branch:** \${GITHUB_REF_NAME:-\${CI_COMMIT_REF_NAME:-$(git branch --show-current)}}

## Summary
- Syntax validation: âœ… PASSED
- Format validation: âœ… PASSED  
- Security scan: âœ… COMPLETED
- Policy validation: âœ… COMPLETED

## Files Checked
$(find . -name "*.hcl" -o -name "*.tf" | wc -l) Terraform/Terragrunt files

## Next Steps
- Review security findings in: security_$TIMESTAMP.json
- Proceed to planning phase if all validations passed
- Full logs available in: validation_$TIMESTAMP.log

EOF

log "ğŸ“‹ Validation summary generated: $LOG_DIR/validation_summary_$TIMESTAMP.md"
log "ğŸ‰ Validation pipeline completed successfully!"

# Upload des logs vers S3 pour archivage
if [[ "${CI:-false}" == "true" ]]; then
    aws s3 cp "$LOG_DIR/" "s3://pipeline-logs-bucket/validation/" --recursive --quiet || true
    log "ğŸ“¤ Logs uploaded to S3"
fi

exit 0
```

---

## Phase 2 : Configuration du Pipeline CI/CD (20 minutes)

### 2.1 GitHub Actions Pipeline
```yaml
# .github/workflows/terragrunt-pipeline.yml
name: ğŸš€ Secure Terragrunt Pipeline

on:
  push:
    branches: [ main, develop, 'feature/*' ]
  pull_request:
    branches: [ main, develop ]
  
  # DÃ©ploiement manuel pour prod
  workflow_dispatch:
    inputs:
      environment:
        description: 'Target environment'
        required: true
        default: 'dev'
        type: choice
        options:
        - dev
        - staging
        - prod
      
      force_deploy:
        description: 'Force deployment (skip safety checks)'
        required: false
        type: boolean
        default: false

env:
  TF_VERSION: '1.6.0'
  TG_VERSION: '0.53.0'
  AWS_DEFAULT_REGION: 'eu-west-1'

jobs:
  # Job 1: Validation complÃ¨te
  validate:
    name: ğŸ” Validate & Security Check
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    outputs:
      validation-status: ${{ steps.validate.outputs.status }}
      
    steps:
    - name: ğŸ“¥ Checkout Code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Pour git-secrets
        
    - name: ğŸ”§ Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TF_VERSION }}
        
    - name: ğŸ”§ Setup Terragrunt
      run: |
        curl -LO "https://github.com/gruntwork-io/terragrunt/releases/download/v${{ env.TG_VERSION }}/terragrunt_linux_amd64"
        chmod +x terragrunt_linux_amd64
        sudo mv terragrunt_linux_amd64 /usr/local/bin/terragrunt
        terragrunt --version
        
    - name: ğŸ” Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_DEFAULT_REGION }}
        
    - name: ğŸ›¡ï¸ Install Security Tools
      run: |
        # Installation tfsec
        curl -s https://raw.githubusercontent.com/aquasecurity/tfsec/master/scripts/install_linux.sh | bash
        
        # Installation git-secrets
        git clone https://github.com/awslabs/git-secrets.git
        cd git-secrets && make install && cd ..
        git secrets --register-aws
        git secrets --install
        
        # Installation Infracost (optionnel)
        if [[ "${{ github.event_name }}" == "pull_request" ]]; then
          curl -fsSL https://raw.githubusercontent.com/infracost/infracost/master/scripts/install.sh | sh
        fi
        
    - name: ğŸ” Run Validation Pipeline
      id: validate
      run: |
        chmod +x scripts/validate.sh
        ./scripts/validate.sh
        echo "status=success" >> $GITHUB_OUTPUT
      env:
        PIPELINE_OWNER: ${{ github.actor }}
        CI_PIPELINE_ID: ${{ github.run_id }}
        
    - name: ğŸ“Š Upload Validation Artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: validation-logs-${{ github.run_id }}
        path: logs/
        retention-days: 30
        
    - name: ğŸ’¬ Comment PR with Validation Results  
      uses: actions/github-script@v7
      if: github.event_name == 'pull_request'
      with:
        script: |
          const fs = require('fs');
          const path = 'logs';
          
          if (fs.existsSync(path)) {
            const files = fs.readdirSync(path);
            const summaryFile = files.find(f => f.includes('validation_summary'));
            
            if (summaryFile) {
              const summary = fs.readFileSync(`${path}/${summaryFile}`, 'utf8');
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: `## ğŸ” Validation Results\n\n${summary}`
              });
            }
          }

  # Job 2: Planification pour tous les environnements
  plan:
    name: ğŸ“‹ Plan All Environments  
    runs-on: ubuntu-latest
    needs: validate
    if: needs.validate.outputs.validation-status == 'success'
    timeout-minutes: 20
    
    strategy:
      matrix:
        environment: [dev, staging, prod]
        
    steps:
    - name: ğŸ“¥ Checkout Code
      uses: actions/checkout@v4
      
    - name: ğŸ”§ Setup Tools
      run: |
        # Setup Terraform
        curl -LO "https://releases.hashicorp.com/terraform/${{ env.TF_VERSION }}/terraform_${{ env.TF_VERSION }}_linux_amd64.zip"
        unzip terraform_${{ env.TF_VERSION }}_linux_amd64.zip
        sudo mv terraform /usr/local/bin/
        
        # Setup Terragrunt  
        curl -LO "https://github.com/gruntwork-io/terragrunt/releases/download/v${{ env.TG_VERSION }}/terragrunt_linux_amd64"
        chmod +x terragrunt_linux_amd64
        sudo mv terragrunt_linux_amd64 /usr/local/bin/terragrunt
        
    - name: ğŸ” Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_DEFAULT_REGION }}
        role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/TerragruntDeployRole-${{ matrix.environment }}
        role-session-name: GitHubActions-${{ github.run_id }}
        
    - name: ğŸ“‹ Generate Terragrunt Plan
      id: plan
      run: |
        cd environments/${{ matrix.environment }}
        
        echo "ğŸ”„ Generating plan for ${{ matrix.environment }}..."
        
        # Plan avec sauvegarde
        terragrunt run-all plan \
          --terragrunt-non-interactive \
          --terragrunt-out-dir ../../plans/${{ matrix.environment }} \
          --terragrunt-log-level info \
          > ../../logs/plan_${{ matrix.environment }}_$(date +%Y%m%d_%H%M%S).log 2>&1
          
        echo "plan-status=success" >> $GITHUB_OUTPUT
      env:
        TF_IN_AUTOMATION: "true"
        
    - name: ğŸ’° Cost Estimation
      if: github.event_name == 'pull_request'
      run: |
        if command -v infracost &> /dev/null; then
          cd environments/${{ matrix.environment }}
          
          infracost breakdown \
            --path . \
            --format json \
            --out-file ../../costs/cost_${{ matrix.environment }}.json || true
            
          infracost diff \
            --path . \
            --format github-comment \
            --repo ${{ github.repository }} \
            --pull-request ${{ github.event.pull_request.number }} \
            --behavior update || true
        fi
        
    - name: ğŸ“¦ Upload Plan Artifacts
      uses: actions/upload-artifact@v3
      with:
        name: terraform-plans-${{ matrix.environment }}-${{ github.run_id }}
        path: |
          plans/${{ matrix.environment }}/
          logs/plan_${{ matrix.environment }}_*.log
        retention-days: 30

  # Job 3: DÃ©ploiement conditionnel
  deploy:
    name: ğŸš€ Deploy Infrastructure
    runs-on: ubuntu-latest
    needs: [validate, plan]
    if: |
      (github.ref == 'refs/heads/main' && github.event_name == 'push') ||
      (github.event_name == 'workflow_dispatch')
    timeout-minutes: 30
    
    environment: 
      name: ${{ github.event.inputs.environment || 'dev' }}
      
    steps:
    - name: ğŸ“¥ Checkout Code
      uses: actions/checkout@v4
      
    - name: ğŸ”§ Setup Tools
      run: |
        curl -LO "https://releases.hashicorp.com/terraform/${{ env.TF_VERSION }}/terraform_${{ env.TF_VERSION }}_linux_amd64.zip"
        unzip terraform_${{ env.TF_VERSION }}_linux_amd64.zip
        sudo mv terraform /usr/local/bin/
        
        curl -LO "https://github.com/gruntwork-io/terragrunt/releases/download/v${{ env.TG_VERSION }}/terragrunt_linux_amd64"
        chmod +x terragrunt_linux_amd64
        sudo mv terragrunt_linux_amd64 /usr/local/bin/terragrunt
        
    - name: ğŸ” Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_DEFAULT_REGION }}
        
    - name: ğŸš€ Deploy Infrastructure
      id: deploy
      run: |
        target_env="${{ github.event.inputs.environment || 'dev' }}"
        
        echo "ğŸš€ Starting deployment to: $target_env"
        
        cd environments/$target_env
        
        # Sauvegarde des logs de dÃ©ploiement
        mkdir -p ../../logs
        
        terragrunt run-all apply \
          --terragrunt-non-interactive \
          --terragrunt-log-level info \
          > ../../logs/deploy_${target_env}_$(date +%Y%m%d_%H%M%S).log 2>&1
          
        echo "deploy-status=success" >> $GITHUB_OUTPUT
        echo "environment=$target_env" >> $GITHUB_OUTPUT
      env:
        TF_IN_AUTOMATION: "true"
        PIPELINE_OWNER: ${{ github.actor }}
        CI_PIPELINE_ID: ${{ github.run_id }}
        
    - name: ğŸ§ª Post-Deploy Validation
      run: |
        echo "ğŸ§ª Running post-deployment tests..."
        
        # Tests de sanitÃ© basiques
        target_env="${{ github.event.inputs.environment || 'dev' }}"
        cd environments/$target_env
        
        # VÃ©rification que les ressources sont crÃ©Ã©es
        terragrunt run-all output > ../../logs/outputs_${target_env}.json
        
        echo "âœ… Post-deployment validation completed"
        
    - name: ğŸ“Š Upload Deploy Artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: deployment-logs-${{ steps.deploy.outputs.environment }}-${{ github.run_id }}
        path: logs/
        retention-days: 90
        
    - name: ğŸ“¤ Archive Logs to S3
      if: always()
      run: |
        # Upload vers S3 pour archivage long terme
        aws s3 cp logs/ s3://pipeline-logs-bucket/deployments/$(date +%Y/%m/%d)/ --recursive --quiet || true
        echo "ğŸ“¤ Logs archived to S3"

  # Job 4: Notification
  notify:
    name: ğŸ“¢ Notify Results
    runs-on: ubuntu-latest
    needs: [validate, plan, deploy]
    if: always()
    
    steps:
    - name: ğŸ“¢ Slack Notification
      uses: 8398a7/action-slack@v3
      if: always()
      with:
        status: ${{ needs.deploy.result || needs.plan.result || needs.validate.result }}
        channel: '#devops'
        webhook_url: ${{ secrets.SLACK_WEBHOOK }}
        fields: repo,message,commit,author,action,eventName,ref,workflow
        custom_payload: |
          {
            attachments: [{
              color: '${{ needs.deploy.result }}' === 'success' ? 'good' : '${{ needs.deploy.result }}' === 'failure' ? 'danger' : 'warning',
              fields: [{
                title: 'Pipeline Result',
                value: '${{ needs.deploy.result || needs.plan.result || needs.validate.result }}',
                short: true
              }, {
                title: 'Environment',
                value: '${{ github.event.inputs.environment || "dev" }}',
                short: true
              }]
            }]
          }
```

### 2.2 Script de Planification
```bash
# scripts/plan-all.sh
#!/bin/bash

set -euo pipefail

echo "ğŸ“‹ Starting comprehensive Terragrunt planning..."

# Configuration
ENVIRONMENTS=("dev" "staging" "prod")
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
PLAN_DIR="plans"
LOG_DIR="logs"

# CrÃ©ation des dossiers
mkdir -p "$PLAN_DIR" "$LOG_DIR"

# Fonction de logging
log() {
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_DIR/plan_all_$TIMESTAMP.log"
}

# Fonction de planification par environnement
plan_environment() {
    local env="$1"
    local plan_log="$LOG_DIR/plan_${env}_$TIMESTAMP.log"
    local plan_output="$PLAN_DIR/${env}"
    
    log "ğŸ“‹ Planning environment: $env"
    
    if [[ ! -d "environments/$env" ]]; then
        log "âš ï¸ Environment directory not found: $env"
        return 1
    fi
    
    cd "environments/$env"
    
    # Nettoyage des anciens plans
    rm -rf "../../$plan_output"
    mkdir -p "../../$plan_output"
    
    # Initialisation de tous les modules
    log "ğŸ”„ Initializing all modules in $env..."
    if ! terragrunt run-all init --terragrunt-non-interactive >> "$plan_log" 2>&1; then
        log "âŒ Failed to initialize modules in $env"
        cd - > /dev/null
        return 1
    fi
    
    # GÃ©nÃ©ration des plans
    log "ğŸ“‹ Generating plans for $env..."
    if terragrunt run-all plan \
        --terragrunt-non-interactive \
        --terragrunt-out-dir "../../$plan_output" \
        --terragrunt-log-level info \
        >> "../../$plan_log" 2>&1; then
        
        log "âœ… Plans generated successfully for $env"
        
        # Analyse des changements
        analyze_plan_changes "$env" "$plan_output"
        
        cd - > /dev/null
        return 0
    else
        log "âŒ Failed to generate plans for $env"
        cd - > /dev/null
        return 1
    fi
}

# Analyse des changements dans les plans
analyze_plan_changes() {
    local env="$1"
    local plan_dir="$2"
    
    log "ğŸ“Š Analyzing plan changes for $env..."
    
    local changes_summary="$LOG_DIR/changes_${env}_$TIMESTAMP.md"
    
    cat << EOF > "$changes_summary"
# Plan Analysis for $env

**Generated:** $(date)
**Environment:** $env

## Summary of Changes
EOF
    
    # Recherche des fichiers de plan
    find "$plan_dir" -name "*.tfplan" | while read -r plan_file; do
        module_name=$(basename "$(dirname "$plan_file")")
        
        echo "### Module: $module_name" >> "$changes_summary"
        
        # Extraction des mÃ©triques de changement
        if terraform show -json "$plan_file" > /dev/null 2>&1; then
            terraform show -json "$plan_file" | jq -r '
                .resource_changes[] |
                select(.change.actions != ["no-op"]) |
                "- \(.change.actions | join(", ")): \(.address)"
            ' >> "$changes_summary" 2>/dev/null || echo "- Analysis failed for $module_name" >> "$changes_summary"
        fi
        
        echo "" >> "$changes_summary"
    done
    
    log "ğŸ“Š Plan analysis saved: $changes_summary"
}

# Validation des prÃ©requis
log "ğŸ” Validating prerequisites..."

# VÃ©rification des outils
for tool in terragrunt terraform aws jq; do
    if ! command -v "$tool" &> /dev/null; then
        log "âŒ Required tool not found: $tool"
        exit 1
    fi
done

# VÃ©rification des credentials AWS
if ! aws sts get-caller-identity > /dev/null 2>&1; then
    log "âŒ AWS credentials not configured"
    exit 1
fi

log "âœ… Prerequisites validated"

# Planification pour tous les environnements
failed_environments=()

for env in "${ENVIRONMENTS[@]}"; do
    if ! plan_environment "$env"; then
        failed_environments+=("$env")
    fi
done

# GÃ©nÃ©ration du rapport global
generate_global_report() {
    local report_file="$LOG_DIR/planning_report_$TIMESTAMP.md"
    
    cat << EOF > "$report_file"
# Global Planning Report

**Date:** $(date)
**Environments:** ${ENVIRONMENTS[*]}
**Branch:** \${GITHUB_REF_NAME:-\${CI_COMMIT_REF_NAME:-$(git branch --show-current 2>/dev/null || echo "unknown")}}
**Commit:** \${GITHUB_SHA:-\${CI_COMMIT_SHA:-$(git rev-parse HEAD 2>/dev/null || echo "unknown")}}

## Planning Results

EOF

    for env in "${ENVIRONMENTS[@]}"; do
        if [[ " ${failed_environments[*]} " =~ " $env " ]]; then
            echo "- âŒ **$env**: FAILED" >> "$report_file"
        else
            echo "- âœ… **$env**: SUCCESS" >> "$report_file"
        fi
    done
    
    cat << EOF >> "$report_file"

## Files Generated

$(find "$PLAN_DIR" -name "*.tfplan" | wc -l) plan files created
$(find "$LOG_DIR" -name "*.log" | wc -l) log files generated

## Security Analysis

EOF

    # IntÃ©gration avec tfsec si disponible
    if command -v tfsec &> /dev/null; then
        echo "### Security Scan Results" >> "$report_file"
        echo '```' >> "$report_file"
        tfsec --format brief . >> "$report_file" 2>/dev/null || echo "Security scan failed" >> "$report_file"
        echo '```' >> "$report_file"
    fi
    
    cat << EOF >> "$report_file"

## Cost Estimation

EOF

    # IntÃ©gration avec Infracost si disponible
    if command -v infracost &> /dev/null; then
        for env in "${ENVIRONMENTS[@]}"; do
            if [[ -d "environments/$env" ]]; then
                echo "### $env Environment" >> "$report_file"
                cd "environments/$env"
                infracost breakdown --path . --format table >> "../../$report_file" 2>/dev/null || echo "Cost analysis failed for $env" >> "../../$report_file"
                cd - > /dev/null
            fi
        done
    else
        echo "Infracost not available - cost estimation skipped" >> "$report_file"
    fi
    
    cat << EOF >> "$report_file"

## Next Steps

1. Review all plan files in the \`$PLAN_DIR\` directory
2. Check security findings and address any issues
3. Validate cost implications before deployment
4. Proceed with deployment on main branch only

## Artifacts

- Plan files: \`$PLAN_DIR/\`
- Logs: \`$LOG_DIR/\`
- Analysis: \`$LOG_DIR/changes_*_$TIMESTAMP.md\`

EOF

    log "ğŸ“‹ Global report generated: $report_file"
}

# GÃ©nÃ©ration du rapport final
generate_global_report

# RÃ©sumÃ© final
if [[ ${#failed_environments[@]} -eq 0 ]]; then
    log "ğŸ‰ All environments planned successfully!"
    
    # Upload vers S3 si en environnement CI
    if [[ "${CI:-false}" == "true" ]]; then
        log "ğŸ“¤ Uploading plans and logs to S3..."
        aws s3 cp "$PLAN_DIR/" "s3://pipeline-artifacts-bucket/plans/$(date +%Y/%m/%d)/" --recursive --quiet || true
        aws s3 cp "$LOG_DIR/" "s3://pipeline-logs-bucket/planning/$(date +%Y/%m/%d)/" --recursive --quiet || true
    fi
    
    exit 0
else
    log "âŒ Planning failed for environments: ${failed_environments[*]}"
    exit 1
fi
```

---

## Phase 3 : SÃ©curisation et Gestion des Secrets (15 minutes)

### 3.1 Configuration des RÃ´les IAM
```hcl
# modules/security/iam-roles.tf
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

locals {
  environments = ["dev", "staging", "prod"]
  
  # Politiques de sÃ©curitÃ© par environnement
  environment_policies = {
    dev = {
      allowed_actions = [
        "ec2:*",
        "rds:*",
        "s3:*",
        "iam:List*",
        "iam:Get*"
      ]
      restricted_actions = []
    }
    staging = {
      allowed_actions = [
        "ec2:*",
        "rds:*",
        "s3:*",
        "iam:List*",
        "iam:Get*"
      ]
      restricted_actions = [
        "iam:Delete*",
        "ec2:TerminateInstances"
      ]
    }
    prod = {
      allowed_actions = [
        "ec2:RunInstances",
        "ec2:DescribeInstances",
        "ec2:ModifyInstanceAttribute",
        "rds:CreateDBInstance",
        "rds:ModifyDBInstance",
        "rds:DescribeDBInstances",
        "s3:GetObject",
        "s3:PutObject",
        "s3:ListBucket"
      ]
      restricted_actions = [
        "iam:*",
        "ec2:TerminateInstances",
        "rds:DeleteDBInstance"
      ]
    }
  }
}

# RÃ´le de dÃ©ploiement pour chaque environnement
resource "aws_iam_role" "terragrunt_deploy_role" {
  for_each = toset(local.environments)
  
  name = "TerragruntDeployRole-${title(each.value)}"
  
  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          # GitHub Actions OIDC
          Federated = "arn:aws:iam::${data.aws_caller_identity.current.account_id}:oidc-provider/token.actions.githubusercontent.com"
        }
        Condition = {
          StringEquals = {
            "token.actions.githubusercontent.com:aud" = "sts.amazonaws.com"
            "token.actions.githubusercontent.com:sub" = [
              "repo:your-org/your-repo:ref:refs/heads/main",
              "repo:your-org/your-repo:ref:refs/heads/develop",
              "repo:your-org/your-repo:pull_request"
            ]
          }
        }
      },
      {
        # Pour les dÃ©ploiements locaux d'urgence
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          AWS = "arn:aws:iam::${data.aws_caller_identity.current.account_id}:user/emergency-deploy-user"
        }
        Condition = {
          StringEquals = {
            "aws:RequestedRegion" = ["eu-west-1", "us-east-1"]
          }
          DateGreaterThan = {
            "aws:CurrentTime" = "2024-01-01T00:00:00Z"
          }
          DateLessThan = {
            "aws:CurrentTime" = "2025-12-31T23:59:59Z"
          }
        }
      }
    ]
  })
  
  tags = {
    Environment = each.value
    Purpose     = "terragrunt-deployment"
    Security    = "high"
  }
}

# Politique de dÃ©ploiement par environnement
resource "aws_iam_policy" "terragrunt_deploy_policy" {
  for_each = toset(local.environments)
  
  name = "TerragruntDeployPolicy-${title(each.value)}"
  
  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = local.environment_policies[each.value].allowed_actions
        Resource = "*"
        Condition = {
          StringEquals = {
            "aws:RequestedRegion" = ["eu-west-1"]
          }
        }
      },
      {
        Effect = "Deny"
        Action = local.environment_policies[each.value].restricted_actions
        Resource = "*"
      },
      {
        # AccÃ¨s au state bucket
        Effect = "Allow"
        Action = [
          "s3:GetObject",
          "s3:PutObject",
          "s3:DeleteObject",
          "s3:ListBucket"
        ]
        Resource = [
          "arn:aws:s3:::secure-terragrunt-state-${each.value}",
          "arn:aws:s3:::secure-terragrunt-state-${each.value}/*"
        ]
      },
      {
        # AccÃ¨s Ã  DynamoDB pour les locks
        Effect = "Allow"
        Action = [
          "dynamodb:GetItem",
          "dynamodb:PutItem",
          "dynamodb:DeleteItem"
        ]
        Resource = "arn:aws:dynamodb:eu-west-1:${data.aws_caller_identity.current.account_id}:table/terragrunt-locks-${each.value}"
      },
      {
        # AccÃ¨s KMS pour le chiffrement
        Effect = "Allow"
        Action = [
          "kms:Decrypt",
          "kms:Encrypt",
          "kms:GenerateDataKey"
        ]
        Resource = "arn:aws:kms:eu-west-1:${data.aws_caller_identity.current.account_id}:key/pipeline-state"
      }
    ]
  })
}

# Attachement des politiques aux rÃ´les
resource "aws_iam_role_policy_attachment" "terragrunt_deploy_policy" {
  for_each = toset(local.environments)
  
  role       = aws_iam_role.terragrunt_deploy_role[each.value].name
  policy_arn = aws_iam_policy.terragrunt_deploy_policy[each.value].arn
}

data "aws_caller_identity" "current" {}

# Outputs pour utilisation dans Terragrunt
output "deploy_role_arns" {
  value = {
    for env in local.environments :
    env => aws_iam_role.terragrunt_deploy_role[env].arn
  }
}
```

### 3.2 Script de DÃ©ploiement SÃ©curisÃ©
```bash
# scripts/deploy.sh
#!/bin/bash

set -euo pipefail

echo "ğŸš€ Starting secure Terragrunt deployment..."

# Configuration
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
LOG_DIR="logs"
DEPLOYMENT_LOG="$LOG_DIR/deployment_$TIMESTAMP.log"

# Arguments
ENVIRONMENT="${1:-dev}"
DRY_RUN="${2:-false}"
FORCE_DEPLOY="${3:-false}"

# Validation des paramÃ¨tres
VALID_ENVIRONMENTS=("dev" "staging" "prod")
if [[ ! " ${VALID_ENVIRONMENTS[*]} " =~ " $ENVIRONMENT " ]]; then
    echo "âŒ Invalid environment: $ENVIRONMENT"
    echo "Valid environments: ${VALID_ENVIRONMENTS[*]}"
    exit 1
fi

# CrÃ©ation du dossier de logs
mkdir -p "$LOG_DIR"

# Fonction de logging avec rotation
log() {
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1" | tee -a "$DEPLOYMENT_LOG"
    
    # Rotation des logs (garder les 10 derniers)
    find "$LOG_DIR" -name "deployment_*.log" -type f | sort | head -n -10 | xargs rm -f || true
}

# Fonction de notification
notify() {
    local status="$1"
    local message="$2"
    
    log "$message"
    
    # Slack notification si configurÃ©
    if [[ -n "${SLACK_WEBHOOK_URL:-}" ]]; then
        curl -s -X POST -H 'Content-type: application/json' \
            --data "{\"text\":\"ğŸš€ Deployment $status: $message\"}" \
            "$SLACK_WEBHOOK_URL" || true
    fi
    
    # Teams notification si configurÃ©
    if [[ -n "${TEAMS_WEBHOOK_URL:-}" ]]; then
        curl -s -H "Content-Type: application/json" -X POST \
            --data "{\"text\":\"ğŸš€ Deployment $status: $message\"}" \
            "$TEAMS_WEBHOOK_URL" || true
    fi
}

# VÃ©rifications de sÃ©curitÃ© prÃ©-dÃ©ploiement
security_checks() {
    log "ğŸ” Running pre-deployment security checks..."
    
    # VÃ©rification des credentials
    local caller_identity
    if ! caller_identity=$(aws sts get-caller-identity 2>/dev/null); then
        log "âŒ AWS credentials not configured or invalid"
        return 1
    fi
    
    local account_id
    account_id=$(echo "$caller_identity" | jq -r '.Account')
    local user_arn
    user_arn=$(echo "$caller_identity" | jq -r '.Arn')
    
    log "ğŸ” Deploying as: $user_arn"
    log "ğŸ¢ AWS Account: $account_id"
    
    # VÃ©rification du rÃ´le pour la production
    if [[ "$ENVIRONMENT" == "prod" ]]; then
        if [[ "$user_arn" != *"TerragruntDeployRole-Prod"* ]] && [[ "$FORCE_DEPLOY" != "true" ]]; then
            log "âŒ Production deployments require specific IAM role"
            log "Current role: $user_arn"
            log "Required role pattern: *TerragruntDeployRole-Prod*"
            return 1
        fi
        
        # VÃ©rification de la branche pour prod
        if [[ "${CI_COMMIT_REF_NAME:-$(git branch --show-current 2>/dev/null)}" != "main" ]] && [[ "$FORCE_DEPLOY" != "true" ]]; then
            log "âŒ Production deployments only allowed from main branch"
            return 1
        fi
    fi
    
    # VÃ©rification des state buckets
    local state_bucket="secure-terragrunt-state-$ENVIRONMENT"
    if ! aws s3 ls "s3://$state_bucket" > /dev/null 2>&1; then
        log "âŒ State bucket not accessible: $state_bucket"
        return 1
    fi
    
    # VÃ©rification du chiffrement KMS
    local kms_key_id="arn:aws:kms:${AWS_DEFAULT_REGION:-eu-west-1}:$account_id:key/pipeline-state"
    if ! aws kms describe-key --key-id "$kms_key_id" > /dev/null 2>&1; then
        log "âš ï¸ KMS key not accessible, state encryption may fail: $kms_key_id"
    fi
    
    log "âœ… Security checks passed"
    return 0
}

# Backup avant dÃ©ploiement
create_backup() {
    log "ğŸ’¾ Creating pre-deployment backup..."
    
    local backup_dir="backups/$ENVIRONMENT/$TIMESTAMP"
    mkdir -p "$backup_dir"
    
    cd "environments/$ENVIRONMENT"
    
    # Backup des Ã©tats actuels
    if terragrunt run-all output -json > "../../$backup_dir/outputs.json" 2>/dev/null; then
        log "âœ… Outputs backup created"
    else
        log "âš ï¸ Could not backup outputs (may be first deployment)"
    fi
    
    # Backup des plans actuels si disponibles
    if [[ -d "../../plans/$ENVIRONMENT" ]]; then
        cp -r "../../plans/$ENVIRONMENT" "../../$backup_dir/plans/"
        log "âœ… Plans backup created"
    fi
    
    cd - > /dev/null
    
    # Compression du backup
    tar -czf "$backup_dir.tar.gz" -C backups "$ENVIRONMENT/$TIMESTAMP"
    rm -rf "$backup_dir"
    
    log "ğŸ’¾ Backup saved: $backup_dir.tar.gz"
}

# DÃ©ploiement avec monitoring
deploy_infrastructure() {
    log "ğŸš€ Starting infrastructure deployment for $ENVIRONMENT..."
    
    cd "environments/$ENVIRONMENT"
    
    # Dry run si demandÃ©
    if [[ "$DRY_RUN" == "true" ]]; then
        log "ğŸ” Running dry-run (plan only)..."
        terragrunt run-all plan \
            --terragrunt-non-interactive \
            --terragrunt-log-level info \
            >> "../../$DEPLOYMENT_LOG" 2>&1
        
        log "âœ… Dry-run completed successfully"
        cd - > /dev/null
        return 0
    fi
    
    # DÃ©ploiement rÃ©el avec timeout
    local start_time
    start_time=$(date +%s)
    
    log "ğŸ”„ Applying infrastructure changes..."
    
    if timeout 1800 terragrunt run-all apply \
        --terragrunt-non-interactive \
        --terragrunt-log-level info \
        >> "../../$DEPLOYMENT_LOG" 2>&1; then
        
        local end_time
        end_time=$(date +%s)
        local duration=$((end_time - start_time))
        
        log "âœ… Deployment completed successfully in ${duration}s"
        
        # Validation post-dÃ©ploiement
        post_deploy_validation
        
        cd - > /dev/null
        return 0
    else
        local end_time
        end_time=$(date +%s)
        local duration=$((end_time - start_time))
        
        log "âŒ Deployment failed after ${duration}s"
        
        # Tentative de rollback si en prod
        if [[ "$ENVIRONMENT" == "prod" ]]; then
            log "ğŸ”„ Attempting automatic rollback for production..."
            # ImplÃ©mentation du rollback ici
        fi
        
        cd - > /dev/null
        return 1
    fi
}

# Validation post-dÃ©ploiement
post_deploy_validation() {
    log "ğŸ§ª Running post-deployment validation..."
    
    # VÃ©rification des outputs
    if terragrunt run-all output > "../../$LOG_DIR/post_deploy_outputs_$TIMESTAMP.json" 2>&1; then
        log "âœ… Infrastructure outputs validated"
    else
        log "âš ï¸ Could not validate outputs"
    fi
    
    # Tests de connectivitÃ© basiques
    local vpc_id
    if vpc_id=$(terragrunt output -raw vpc_id 2>/dev/null); then
        if aws ec2 describe-vpcs --vpc-ids "$vpc_id" > /dev/null 2>&1; then
            log "âœ… VPC connectivity validated: $vpc_id"
        else
            log "âŒ VPC validation failed: $vpc_id"
            return 1
        fi
    fi
    
    # VÃ©rification des tags de sÃ©curitÃ©
    local resources_without_tags
    if resources_without_tags=$(aws resourcegroupstaggingapi get-resources \
        --region "${AWS_DEFAULT_REGION:-eu-west-1}" \
        --resource-type-filters "AWS::EC2::Instance" "AWS::RDS::DBInstance" \
        --query 'ResourceTagMappingList[?!Tags[?Key==`Environment`]]' \
        --output text 2>/dev/null); then
        
        if [[ -n "$resources_without_tags" ]]; then
            log "âš ï¸ Found resources without required tags"
        else
            log "âœ… All resources properly tagged"
        fi
    fi
    
    log "âœ… Post-deployment validation completed"
}

# Main execution flow
main() {
    notify "STARTED" "Deployment started for $ENVIRONMENT environment"
    
    # VÃ©rifications de sÃ©curitÃ©
    if ! security_checks; then
        notify "FAILED" "Security checks failed for $ENVIRONMENT"
        exit 1
    fi
    
    # Backup
    if [[ "$ENVIRONMENT" == "prod" ]] || [[ "$ENVIRONMENT" == "staging" ]]; then
        create_backup
    fi
    
    # DÃ©ploiement
    if deploy_infrastructure; then
        notify "SUCCESS" "Deployment completed successfully for $ENVIRONMENT"
        
        # Archive des logs
        log "ğŸ“¤ Archiving deployment logs..."
        aws s3 cp "$DEPLOYMENT_LOG" \
            "s3://pipeline-logs-bucket/deployments/$(date +%Y/%m/%d)/" \
            --quiet || log "âš ï¸ Failed to archive logs to S3"
        
        exit 0
    else
        notify "FAILED" "Deployment failed for $ENVIRONMENT"
        
        # Archive des logs d'erreur
        aws s3 cp "$DEPLOYMENT_LOG" \
            "s3://pipeline-logs-bucket/failures/$(date +%Y/%m/%d)/" \
            --quiet || log "âš ï¸ Failed to archive failure logs"
        
        exit 1
    fi
}

# Gestion des signaux pour cleanup
cleanup() {
    log "ğŸ§¹ Cleaning up deployment process..."
    # Nettoyage des processus en cours si nÃ©cessaire
    exit 130
}

trap cleanup SIGINT SIGTERM

# Affichage de la configuration
log "ğŸ“‹ Deployment Configuration:"
log "  Environment: $ENVIRONMENT"
log "  Dry Run: $DRY_RUN"
log "  Force Deploy: $FORCE_DEPLOY"
log "  Branch: ${CI_COMMIT_REF_NAME:-$(git branch --show-current 2>/dev/null || echo 'unknown')}"
log "  Commit: ${CI_COMMIT_SHA:-$(git rev-parse HEAD 2>/dev/null || echo 'unknown')}"

# ExÃ©cution principale
main "$@"
```

---

## Phase 4 : Tests, Logs et Monitoring (10 minutes)

### 4.1 Script de Sauvegarde des Logs
```bash
# scripts/save-logs.sh
#!/bin/bash

set -euo pipefail

echo "ğŸ“Š Starting log archival and monitoring setup..."

# Configuration
LOG_DIR="logs"
BACKUP_DIR="log-backups"
S3_BUCKET="pipeline-logs-bucket"
RETENTION_DAYS=90
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")

# Fonction de logging
log() {
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1"
}

# CrÃ©ation des dossiers
mkdir -p "$BACKUP_DIR"

# Archivage des logs locaux
archive_local_logs() {
    log "ğŸ“¦ Archiving local logs..."
    
    if [[ -d "$LOG_DIR" ]]; then
        # Compression des logs
        tar -czf "$BACKUP_DIR/logs_$TIMESTAMP.tar.gz" -C "$LOG_DIR" .
        
        local archive_size
        archive_size=$(du -h "$BACKUP_DIR/logs_$TIMESTAMP.tar.gz" | cut -f1)
        log "âœ… Local logs archived: $archive_size"
        
        # Nettoyage des anciens logs locaux
        find "$LOG_DIR" -name "*.log" -type f -mtime +7 -delete || true
        log "ğŸ§¹ Old local logs cleaned (>7 days)"
    else
        log "âš ï¸ No local logs directory found"
    fi
}

# Upload vers S3 avec structure organisÃ©e
upload_to_s3() {
    log "ğŸ“¤ Uploading logs to S3..."
    
    local year month day
    year=$(date +%Y)
    month=$(date +%m)
    day=$(date +%d)
    
    # Structure: s3://bucket/type/year/month/day/
    local s3_prefix="s3://$S3_BUCKET"
    
    # Upload des logs par type
    if [[ -f "$LOG_DIR/validation_"*.log ]]; then
        aws s3 cp "$LOG_DIR/" "$s3_prefix/validation/$year/$month/$day/" \
            --recursive --include "validation_*.log" --quiet || log "âš ï¸ Failed to upload validation logs"
    fi
    
    if [[ -f "$LOG_DIR/plan_"*.log ]]; then
        aws s3 cp "$LOG_DIR/" "$s3_prefix/planning/$year/$month/$day/" \
            --recursive --include "plan_*.log" --quiet || log "âš ï¸ Failed to upload planning logs"
    fi
    
    if [[ -f "$LOG_DIR/deployment_"*.log ]]; then
        aws s3 cp "$LOG_DIR/" "$s3_prefix/deployments/$year/$month/$day/" \
            --recursive --include "deployment_*.log" --quiet || log "âš ï¸ Failed to upload deployment logs"
    fi
    
    # Upload des rapports
    if [[ -f "$LOG_DIR/"*"_report_"*.md ]]; then
        aws s3 cp "$LOG_DIR/" "$s3_prefix/reports/$year/$month/$day/" \
            --recursive --include "*_report_*.md" --quiet || log "âš ï¸ Failed to upload reports"
    fi
    
    # Upload des backups compressÃ©s
    if [[ -f "$BACKUP_DIR/logs_$TIMESTAMP.tar.gz" ]]; then
        aws s3 cp "$BACKUP_DIR/logs_$TIMESTAMP.tar.gz" \
            "$s3_prefix/archives/$year/$month/$day/" --quiet || log "âš ï¸ Failed to upload log archives"
    fi
    
    log "âœ… Logs uploaded to S3"
}

# Configuration des mÃ©triques CloudWatch
setup_cloudwatch_metrics() {
    log "ğŸ“Š Setting up CloudWatch metrics..."
    
    # MÃ©trique personnalisÃ©e pour les dÃ©ploiements
    aws cloudwatch put-metric-data \
        --namespace "Terragrunt/Pipeline" \
        --metric-data \
        MetricName=DeploymentCount,Value=1,Unit=Count,Dimensions=Environment="${ENVIRONMENT:-unknown}" \
        || log "âš ï¸ Failed to send CloudWatch metrics"
    
    # MÃ©triques de performance si disponibles
    if [[ -f "$LOG_DIR/performance_metrics.json" ]]; then
        local duration
        duration=$(jq -r '.deployment_duration // 0' "$LOG_DIR/performance_metrics.json" 2>/dev/null || echo "0")
        
        aws cloudwatch put-metric-data \
            --namespace "Terragrunt/Pipeline" \
            --metric-data \
            MetricName=DeploymentDuration,Value="$duration",Unit=Seconds,Dimensions=Environment="${ENVIRONMENT:-unknown}" \
            || log "âš ï¸ Failed to send performance metrics"
    fi
    
    log "âœ… CloudWatch metrics configured"
}

# Nettoyage des anciens logs dans S3
cleanup_old_s3_logs() {
    log "ğŸ§¹ Cleaning up old S3 logs..."
    
    # Utilisation du lifecycle policy S3 pour automatiser
    # Ici on configure juste la politique si elle n'existe pas
    
    local lifecycle_config=$(cat <<EOF
{
    "Rules": [
        {
            "ID": "DeleteOldLogs",
            "Status": "Enabled",
            "Filter": {"Prefix": ""},
            "Expiration": {"Days": $RETENTION_DAYS},
            "NoncurrentVersionExpiration": {"NoncurrentDays": 30}
        }
    ]
}
EOF
)
    
    echo "$lifecycle_config" > /tmp/lifecycle.json
    
    if aws s3api put-bucket-lifecycle-configuration \
        --bucket "$S3_BUCKET" \
        --lifecycle-configuration file:///tmp/lifecycle.json 2>/dev/null; then
        log "âœ… S3 lifecycle policy configured (${RETENTION_DAYS} days retention)"
    else
        log "âš ï¸ Could not configure S3 lifecycle policy"
    fi
    
    rm -f /tmp/lifecycle.json
}

# GÃ©nÃ©ration des dashboards CloudWatch
create_cloudwatch_dashboard() {
    log "ğŸ“ˆ Creating CloudWatch dashboard..."
    
    local dashboard_body=$(cat <<'EOF'
{
    "widgets": [
        {
            "type": "metric",
            "properties": {
                "metrics": [
                    ["Terragrunt/Pipeline", "DeploymentCount", "Environment", "dev"],
                    [".", ".", ".", "staging"],
                    [".", ".", ".", "prod"]
                ],
                "period": 300,
                "stat": "Sum",
                "region": "eu-west-1",
                "title": "Deployments by Environment"
            }
        },
        {
            "type": "metric",
            "properties": {
                "metrics": [
                    ["Terragrunt/Pipeline", "DeploymentDuration", "Environment", "dev"],
                    [".", ".", ".", "staging"],
                    [".", ".", ".", "prod"]
                ],
                "period": 300,
                "stat": "Average",
                "region": "eu-west-1",
                "title": "Average Deployment Duration"
            }
        }
    ]
}
EOF
)
    
    aws cloudwatch put-dashboard \
        --dashboard-name "TerragruntPipeline" \
        --dashboard-body "$dashboard_body" \
        > /dev/null 2>&1 && log "âœ… CloudWatch dashboard created" || log "âš ï¸ Failed to create dashboard"
}

# Configuration des alertes
setup_cloudwatch_alarms() {
    log "ğŸš¨ Setting up CloudWatch alarms..."
    
    # Alarme pour les Ã©checs de dÃ©ploiement
    aws cloudwatch put-metric-alarm \
        --alarm-name "TerragruntDeploymentFailures" \
        --alarm-description "Alert on Terragrunt deployment failures" \
        --metric-name "DeploymentFailures" \
        --namespace "Terragrunt/Pipeline" \
        --statistic "Sum" \
        --period 300 \
        --threshold 1 \
        --comparison-operator "GreaterThanOrEqualToThreshold" \
        --evaluation-periods 1 \
        --alarm-actions "arn:aws:sns:eu-west-1:${AWS_ACCOUNT_ID:-123456789012}:terragrunt-alerts" \
        > /dev/null 2>&1 && log "âœ… Deployment failure alarm configured" || log "âš ï¸ Failed to create alarm"
    
    # Alarme pour les dÃ©ploiements longs
    aws cloudwatch put-metric-alarm \
        --alarm-name "TerragruntLongDeployments" \
        --alarm-description "Alert on long-running deployments" \
        --metric-name "DeploymentDuration" \
        --namespace "Terragrunt/Pipeline" \
        --statistic "Average" \
        --period 300 \
        --threshold 1800 \
        --comparison-operator "GreaterThanThreshold" \
        --evaluation-periods 2 \
        --alarm-actions "arn:aws:sns:eu-west-1:${AWS_ACCOUNT_ID:-123456789012}:terragrunt-alerts" \
        > /dev/null 2>&1 && log "âœ… Long deployment alarm configured" || log "âš ï¸ Failed to create alarm"
}

# GÃ©nÃ©ration du rapport de logs
generate_log_report() {
    log "ğŸ“‹ Generating log analysis report..."
    
    local report_file="$LOG_DIR/log_analysis_$TIMESTAMP.md"
    
    cat << EOF > "$report_file"
# Log Analysis Report

**Generated:** $(date)
**Pipeline Run:** \${CI_PIPELINE_ID:-manual}
**Branch:** \${CI_COMMIT_REF_NAME:-$(git branch --show-current 2>/dev/null || echo 'unknown')}

## Log Summary

EOF
    
    # Analyse des logs de validation
    if [[ -f "$LOG_DIR"/validation_*.log ]]; then
        local validation_logs
        validation_logs=$(find "$LOG_DIR" -name "validation_*.log" -type f | wc -l)
        echo "- **Validation Logs:** $validation_logs files" >> "$report_file"
        
        # Extraction des erreurs
        local error_count
        error_count=$(grep -c "âŒ\|ERROR\|FAILED" "$LOG_DIR"/validation_*.log 2>/dev/null || echo "0")
        echo "- **Validation Errors:** $error_count" >> "$report_file"
    fi
    
    # Analyse des logs de planification
    if [[ -f "$LOG_DIR"/plan_*.log ]]; then
        local plan_logs
        plan_logs=$(find "$LOG_DIR" -name "plan_*.log" -type f | wc -l)
        echo "- **Planning Logs:** $plan_logs files" >> "$report_file"
        
        # Extraction des changements
        local changes_count
        changes_count=$(grep -c "Plan:\|will be\|# will be" "$LOG_DIR"/plan_*.log 2>/dev/null || echo "0")
        echo "- **Infrastructure Changes:** $changes_count" >> "$report_file"
    fi
    
    # Analyse des logs de dÃ©ploiement
    if [[ -f "$LOG_DIR"/deployment_*.log ]]; then
        local deploy_logs
        deploy_logs=$(find "$LOG_DIR" -name "deployment_*.log" -type f | wc -l)
        echo "- **Deployment Logs:** $deploy_logs files" >> "$report_file"
        
        # Extraction de la durÃ©e
        local duration
        duration=$(grep "completed successfully in" "$LOG_DIR"/deployment_*.log 2>/dev/null | tail -1 | grep -o '[0-9]\+s' || echo "unknown")
        echo "- **Last Deployment Duration:** $duration" >> "$report_file"
    fi
    
    cat << EOF >> "$report_file"

## Storage Locations

- **Local Archives:** \`$BACKUP_DIR/logs_$TIMESTAMP.tar.gz\`
- **S3 Validation:** \`s3://$S3_BUCKET/validation/$(date +%Y/%m/%d)/\`
- **S3 Planning:** \`s3://$S3_BUCKET/planning/$(date +%Y/%m/%d)/\`
- **S3 Deployments:** \`s3://$S3_BUCKET/deployments/$(date +%Y/%m/%d)/\`

## Monitoring

- **CloudWatch Dashboard:** [TerragruntPipeline](https://console.aws.amazon.com/cloudwatch/home?region=eu-west-1#dashboards:name=TerragruntPipeline)
- **Log Retention:** $RETENTION_DAYS days
- **Alerts Configured:** Deployment failures, Long deployments

## Recommendations

EOF
    
    # Recommandations basÃ©es sur l'analyse
    if [[ -f "$LOG_DIR"/validation_*.log ]]; then
        local security_warnings
        security_warnings=$(grep -c "WARN\|âš ï¸" "$LOG_DIR"/validation_*.log 2>/dev/null || echo "0")
        if [[ "$security_warnings" -gt 0 ]]; then
            echo "- âš ï¸ Review $security_warnings security warnings in validation logs" >> "$report_file"
        fi
    fi
    
    local total_log_size
    total_log_size=$(du -sh "$LOG_DIR" 2>/dev/null | cut -f1 || echo "unknown")
    echo "- ğŸ“Š Total log size: $total_log_size" >> "$report_file"
    
    if [[ -f "$LOG_DIR"/security_*.json ]]; then
        echo "- ğŸ” Security scan results available for review" >> "$report_file"
    fi
    
    echo "- ğŸ”„ Logs automatically archived to S3 with $RETENTION_DAYS days retention" >> "$report_file"
    
    log "ğŸ“‹ Log analysis report generated: $report_file"
}

# Main execution
main() {
    log "ğŸ“Š Starting log management and monitoring setup..."
    
    # Archivage local
    archive_local_logs
    
    # Upload vers S3
    if aws sts get-caller-identity > /dev/null 2>&1; then
        upload_to_s3
        setup_cloudwatch_metrics
        cleanup_old_s3_logs
        create_cloudwatch_dashboard
        setup_cloudwatch_alarms
    else
        log "âš ï¸ AWS credentials not available, skipping S3 and CloudWatch operations"
    fi
    
    # GÃ©nÃ©ration du rapport
    generate_log_report
    
    log "âœ… Log management and monitoring setup completed"
}

# VÃ©rification des prÃ©requis
if ! command -v aws &> /dev/null; then
    log "âŒ AWS CLI not found"
    exit 1
fi

if ! command -v jq &> /dev/null; then
    log "âš ï¸ jq not found, some features will be limited"
fi

# ExÃ©cution
main "$@"
```

### 4.2 Configuration des Tests d'IntÃ©gration
```yaml
# .github/workflows/integration-tests.yml
name: ğŸ§ª Integration Tests

on:
  schedule:
    - cron: '0 2 * * *'  # Tests quotidiens Ã  2h
  workflow_dispatch:
    inputs:
      test_environment:
        description: 'Environment to test'
        required: true
        default: 'dev'
        type: choice
        options: [dev, staging, prod]

env:
  AWS_DEFAULT_REGION: 'eu-west-1'

jobs:
  integration-tests:
    name: ğŸ§ª Run Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 45
    
    strategy:
      matrix:
        test-suite: [connectivity, security, performance, compliance]
        environment: ${{ fromJSON('["dev", "staging"]') }}  # Pas de tests auto sur prod
    
    steps:
    - name: ğŸ“¥ Checkout Code
      uses: actions/checkout@v4
      
    - name: ğŸ” Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_DEFAULT_REGION }}
        
    - name: ğŸ§ª Run Test Suite
      run: |
        case "${{ matrix.test-suite }}" in
          connectivity)
            echo "ğŸŒ Testing infrastructure connectivity..."
            cd environments/${{ matrix.environment }}
            
            # Test VPC connectivity
            vpc_id=$(terragrunt output -raw vpc_id 2>/dev/null || echo "")
            if [[ -n "$vpc_id" ]]; then
              aws ec2 describe-vpcs --vpc-ids "$vpc_id" --query 'Vpcs[0].State' --output text
              echo "âœ… VPC connectivity test passed"
            else
              echo "âŒ VPC not found"
              exit 1
            fi
            
            # Test web server connectivity
            if public_ip=$(terragrunt output -raw web_server_ip 2>/dev/null); then
              if curl -s --max-time 10 "http://$public_ip" > /dev/null; then
                echo "âœ… Web server connectivity test passed"
              else
                echo "âŒ Web server not accessible"
                exit 1
              fi
            fi
            ;;
            
          security)
            echo "ğŸ”’ Running security tests..."
            
            # VÃ©rification des security groups
            cd environments/${{ matrix.environment }}
            sg_ids=$(aws ec2 describe-security-groups --query 'SecurityGroups[?VpcId!=null].GroupId' --output text)
            
            for sg_id in $sg_ids; do
              # VÃ©rifier qu'aucun SG n'autorise 0.0.0.0/0 sur tous les ports
              open_rules=$(aws ec2 describe-security-groups --group-ids "$sg_id" \
                --query 'SecurityGroups[0].IpPermissions[?IpRanges[?CidrIp==`0.0.0.0/0`] && (FromPort==null || FromPort==`0`)]' \
                --output text)
              
              if [[ -n "$open_rules" ]]; then
                echo "âŒ Security group $sg_id has overly permissive rules"
                exit 1
              fi
            done
            
            echo "âœ… Security tests passed"
            ;;
            
          performance)
            echo "âš¡ Running performance tests..."
            
            start_time=$(date +%s)
            cd environments/${{ matrix.environment }}
            
            # Test de plan rapide
            terragrunt run-all plan --terragrunt-non-interactive > /dev/null
            
            end_time=$(date +%s)
            duration=$((end_time - start_time))
            
            # Le plan ne devrait pas prendre plus de 5 minutes
            if [[ $duration -gt 300 ]]; then
              echo "âŒ Planning took too long: ${duration}s"
              exit 1
            fi
            
            echo "âœ… Performance tests passed (${duration}s)"
            ;;
            
          compliance)
            echo "ğŸ“‹ Running compliance tests..."
            
            # VÃ©rification des tags obligatoires
            resources=$(aws resourcegroupstaggingapi get-resources \
              --region ${{ env.AWS_DEFAULT_REGION }} \
              --query 'ResourceTagMappingList[?!Tags[?Key==`Environment`]]' \
              --output text)
            
            if [[ -n "$resources" ]]; then
              echo "âŒ Found resources without Environment tag"
              echo "$resources"
              exit 1
            fi
            
            echo "âœ… Compliance tests passed"
            ;;
        esac
        
    - name: ğŸ“Š Upload Test Results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: integration-test-results-${{ matrix.test-suite }}-${{ matrix.environment }}
        path: test-results/
        retention-days: 7
```

### 4.3 Monitoring et Alertes AvancÃ©es
```hcl
# modules/monitoring/cloudwatch.tf
resource "aws_cloudwatch_log_group" "pipeline_logs" {
  name              = "/aws/terragrunt/pipeline"
  retention_in_days = 90
  
  tags = {
    Environment = "pipeline"
    Purpose     = "terragrunt-logging"
  }
}

resource "aws_cloudwatch_log_stream" "validation_logs" {
  name           = "validation"
  log_group_name = aws_cloudwatch_log_group.pipeline_logs.name
}

resource "aws_cloudwatch_log_stream" "deployment_logs" {
  name           = "deployment"  
  log_group_name = aws_cloudwatch_log_group.pipeline_logs.name
}

# MÃ©triques personnalisÃ©es
resource "aws_cloudwatch_log_metric_filter" "deployment_errors" {
  name           = "TerragruntDeploymentErrors"
  pattern        = "[timestamp, request_id, ERROR]"
  log_group_name = aws_cloudwatch_log_group.pipeline_logs.name

  metric_transformation {
    name      = "DeploymentErrors"
    namespace = "Terragrunt/Pipeline"
    value     = "1"
  }
}

resource "aws_cloudwatch_log_metric_filter" "security_violations" {
  name           = "TerragruntSecurityViolations"
  pattern        = "[timestamp, request_id, SECURITY_VIOLATION]"
  log_group_name = aws_cloudwatch_log_group.pipeline_logs.name

  metric_transformation {
    name      = "SecurityViolations"
    namespace = "Terragrunt/Pipeline" 
    value     = "1"
  }
}

# SNS pour les alertes
resource "aws_sns_topic" "pipeline_alerts" {
  name = "terragrunt-pipeline-alerts"
  
  tags = {
    Environment = "pipeline"
    Purpose     = "alerting"
  }
}

resource "aws_sns_topic_subscription" "email_alerts" {
  topic_arn = aws_sns_topic.pipeline_alerts.arn
  protocol  = "email"
  endpoint  = var.alert_email
}

resource "aws_sns_topic_subscription" "slack_alerts" {
  count     = var.slack_webhook_url != "" ? 1 : 0
  topic_arn = aws_sns_topic.pipeline_alerts.arn
  protocol  = "https"
  endpoint  = var.slack_webhook_url
}

# Alarmes critiques
resource "aws_cloudwatch_metric_alarm" "deployment_failures" {
  alarm_name          = "terragrunt-deployment-failures"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = "1"
  metric_name         = "DeploymentErrors"
  namespace           = "Terragrunt/Pipeline"
  period              = "300"
  statistic           = "Sum"
  threshold           = "0"
  alarm_description   = "This metric monitors deployment failures"
  alarm_actions       = [aws_sns_topic.pipeline_alerts.arn]
  
  tags = {
    Environment = "pipeline"
    Criticality = "high"
  }
}

resource "aws_cloudwatch_metric_alarm" "security_violations" {
  alarm_name          = "terragrunt-security-violations"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = "1"
  metric_name         = "SecurityViolations"
  namespace           = "Terragrunt/Pipeline"
  period              = "300"
  statistic           = "Sum"
  threshold           = "0"
  alarm_description   = "This metric monitors security violations"
  alarm_actions       = [aws_sns_topic.pipeline_alerts.arn]
  
  tags = {
    Environment = "pipeline"
    Criticality = "critical"
  }
}

# Dashboard complet
resource "aws_cloudwatch_dashboard" "pipeline_dashboard" {
  dashboard_name = "TerragruntPipeline"

  dashboard_body = jsonencode({
    widgets = [
      {
        type   = "metric"
        x      = 0
        y      = 0
        width  = 12
        height = 6

        properties = {
          metrics = [
            ["Terragrunt/Pipeline", "DeploymentCount", "Environment", "dev"],
            [".", ".", ".", "staging"],
            [".", ".", ".", "prod"]
          ]
          view    = "timeSeries"
          stacked = false
          region  = "eu-west-1"
          title   = "Deployments by Environment"
          period  = 300
        }
      },
      {
        type   = "metric"
        x      = 0
        y      = 6
        width  = 12
        height = 6

        properties = {
          metrics = [
            ["Terragrunt/Pipeline", "DeploymentDuration", "Environment", "dev"],
            [".", ".", ".", "staging"],
            [".", ".", ".", "prod"]
          ]
          view    = "timeSeries"
          stacked = false
          region  = "eu-west-1"
          title   = "Deployment Duration"
          period  = 300
        }
      },
      {
        type   = "metric"
        x      = 0
        y      = 12
        width  = 12
        height = 6

        properties = {
          metrics = [
            ["Terragrunt/Pipeline", "DeploymentErrors"],
            [".", "SecurityViolations"]
          ]
          view    = "timeSeries"
          stacked = false
          region  = "eu-west-1"
          title   = "Errors and Security Violations"
          period  = 300
        }
      }
    ]
  })
}

variable "alert_email" {
  description = "Email address for pipeline alerts"
  type        = string
}

variable "slack_webhook_url" {
  description = "Slack webhook URL for notifications"
  type        = string
  default     = ""
}

output "log_group_name" {
  value = aws_cloudwatch_log_group.pipeline_logs.name
}

output "sns_topic_arn" {
  value = aws_sns_topic.pipeline_alerts.arn
}
```

## ğŸ¯ CritÃ¨res de RÃ©ussite de l'Exercice

### âœ… Validation (Phase 1 - 15 points)
- [ ] Syntax validation automatisÃ©e pour tous les fichiers HCL/TF
- [ ] Scan de sÃ©curitÃ© avec tfsec sans erreurs critiques
- [ ] Validation des formats et conventions de nommage
- [ ] DÃ©tection des secrets dans le code

### âœ… Planification (Phase 2 - 20 points)
- [ ] Plans gÃ©nÃ©rÃ©s pour tous les environnements
- [ ] Estimation des coÃ»ts intÃ©grÃ©e
- [ ] Analyse des changements avec mÃ©triques
- [ ] Sauvegarde des plans avec horodatage

### âœ… SÃ©curitÃ© (Phase 3 - 15 points)  
- [ ] RÃ´les IAM configurÃ©s avec permissions minimales
- [ ] Assume role fonctionnel pour chaque environnement
- [ ] Validation des credentials et des branches
- [ ] Chiffrement des states avec KMS

### âœ… DÃ©ploiement (Phase 4 - 10 points)
- [ ] DÃ©ploiement conditionnel sur la branche main
- [ ] Logs complets sauvegardÃ©s en S3
- [ ] Notifications en cas de succÃ¨s/Ã©chec
- [ ] Rollback automatique en cas d'erreur (prod)

## ğŸš€ Pour aller plus loin vous pouvez ajouter les Ã©lÃ©ments suivants :

### ğŸ”’ SÃ©curitÃ© AvancÃ©e
- IntÃ©gration avec HashiCorp Vault pour les secrets
- Scan de conformitÃ© avec Open Policy Agent (OPA)
- Signature des artefacts avec Sigstore/Cosign
- Audit trail complet dans CloudTrail

### ğŸ“Š ObservabilitÃ©
- MÃ©triques custom dans CloudWatch
- Dashboard Grafana avec Prometheus
- Alerting intelligent avec PagerDuty
- Traces distribuÃ©es avec AWS X-Ray

### ğŸ”„ Automatisation
- Auto-scaling du pipeline selon la charge
- DÃ©ploiements canary automatisÃ©s  
- Tests de regression automatisÃ©s
- Cleanup automatique des ressources de test

## ğŸ“š Points ClÃ©s Appris

1. **Pipeline sÃ©curisÃ©** : Validation systÃ©matique avant dÃ©ploiement
2. **Gestion des secrets** : RÃ´les IAM et assume role
3. **ObservabilitÃ©** : Logs centralisÃ©s et monitoring
4. **Automatisation** : DÃ©ploiements conditionnels et notifications
5. **Gouvernance** : Politiques de sÃ©curitÃ© et conformitÃ©
